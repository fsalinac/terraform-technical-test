# üß™ Terraform + Kubernetes Technical Test

## üìå Autor  
**Flavio Mois√©s Salinas Cari**  
Repositorio p√∫blico: [github.com/fsalinac/terraform-technical-test](https://github.com/fsalinac/terraform-technical-test)

---

## ‚úÖ Objetivo  
El objetivo de esta prueba fue demostrar la capacidad para gestionar infraestructura en Kubernetes usando exclusivamente Terraform, incluyendo la modularizaci√≥n, el despliegue de aplicaciones en contenedores, el uso de Ingress y la integraci√≥n con Harbor como registry.

---

## üìÅ Estructura del Proyecto

```
terraform_technical_flavio.salinas/
‚îÇ
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îî‚îÄ‚îÄ harbor/
‚îÇ       ‚îú‚îÄ‚îÄ main.tf
‚îÇ       ‚îú‚îÄ‚îÄ variables.tf
‚îÇ       ‚îî‚îÄ‚îÄ outputs.tf
```

---

## üöÄ C√≥mo levantar cada componente

### 1. Clonar el repositorio

```bash
git clone https://github.com/fsalinac/terraform-technical-test.git
cd terraform-technical-test
```

### 2. Configurar tus variables

Edita `variables.tf` para personalizar par√°metros como el namespace, nombre de imagen y host:

```hcl
variable "namespace" {
  default = "flavio-salinas"
}

variable "app_name" {
  default = "testapp"
}

variable "image_repository" {
  default = "fmsalinasc/testapp:test"
}

variable "ingress_host" {
  default = "app.miking.duckdns.org"
}
```

### 3. Inicializar Terraform

```bash
terraform init
```

### 4. Aplicar infraestructura de la app

```bash
terraform apply -target=module.app
```

> **Nota:** El m√≥dulo Harbor fue estructurado, pero no se despleg√≥ ya que no ten√≠a acceso a la nube para exponer un LoadBalancer real para Harbor. Como soluci√≥n, utilic√© **DockerHub** para almacenar la imagen y no interrumpir la entrega funcional de la prueba.

---

## ‚úÖ C√≥mo probar el funcionamiento del sistema

### Desde dentro del cl√∫ster (valida el despliegue y comunicaci√≥n):

```bash
kubectl exec -n flavio-salinas -it $(kubectl get pod -n flavio-salinas -l app=testapp -o jsonpath="{.items[0].metadata.name}") -- curl localhost:80
```

### Desde fuera del cl√∫ster:

```bash
curl -H "Host: app.miking.duckdns.org" http://<NODE_INTERNAL_IP>:30593
```

Ejemplo con valores reales de mi entorno:

```bash
curl -H "Host: app.miking.duckdns.org" http://10.0.0.100:30593
```

---

## üìå Supuestos Realizados

- No se ten√≠a acceso total a la nube para exponer puertos p√∫blicos, por lo que el Ingress se configur√≥ usando **NodePort**, y se valid√≥ con la IP interna del nodo.
- En lugar de Harbor, se utiliz√≥ DockerHub para almacenar la imagen (`fmsalinasc/testapp:test`) y no bloquear el flujo de trabajo.
- La aplicaci√≥n contenida en `app.py` corre en el puerto 80, lo cual fue validado dentro del contenedor y reflejado en el deployment.
- No se incluy√≥ Let‚Äôs Encrypt por no contar con un DNS p√∫blico real con control completo ni acceso a puertos 443 abiertos.
- La base de datos Redis y MariaDB ya estaban preconfiguradas por el entorno (seg√∫n indicaciones del PDF).

---

## üß† Componentes desplegados exitosamente

| Componente                     | Estado       | Notas                                                                 |
|-------------------------------|--------------|-----------------------------------------------------------------------|
| Namespace `flavio-salinas`    | ‚úÖ Completado | Cumple el requerimiento de usar nombre personalizado.                |
| App desplegada (Deployment)   | ‚úÖ Completado | Imagen almacenada en DockerHub.                                      |
| Servicio (ClusterIP)          | ‚úÖ Completado | Conectado correctamente al pod.                                      |
| Ingress Controller            | ‚úÖ Completado | Funciona correctamente con reglas de host/path.                      |
| Comunicaci√≥n interna          | ‚úÖ Validada   | Se prob√≥ conexi√≥n a `localhost:80` desde dentro del pod.             |
| Comunicaci√≥n externa (curl)   | ‚ö† Parcial    | Funciona v√≠a NodePort, pero no se expuso con LoadBalancer externo.   |
| Harbor                        | ‚ö† No desplegado | Se dise√±√≥ el m√≥dulo, pero no se despleg√≥ por falta de acceso externo. |

---

## üß© Consideraciones Finales

Esta soluci√≥n fue dise√±ada y ejecutada exclusivamente por m√≠, respetando los l√≠mites del entorno disponible. Los elementos que no se completaron fueron √∫nicamente por falta de acceso de red para exposici√≥n p√∫blica, no por desconocimiento t√©cnico .
El uso de DockerHub se dio para mitigar errores internos al momento de desplegar Hardor dado que este presento problemas de recursos al momento de su despliegue y por las limitantes del tiempo y la idea de lograr el objetivo final (tener una aplicaciones funcional y demostrar los conocimientos requeridos en la prueba). 
No se logro exponer la comunicacion externa debido a limitaciones cloud . Durante la implementaci√≥n de la prueba, se configur√≥ el Ingress Controller (ingress-nginx) con tipo NodePort, exponiendo los puertos necesarios (80 y 443) en el cl√∫ster. Internamente, se valid√≥ correctamente el acceso a la aplicaci√≥n mediante peticiones HTTP direccionadas al Node IP y NodePort, lo cual demuestra que la infraestructura y la aplicaci√≥n est√°n operativas dentro del entorno Kubernetes.
Sin embargo, para que la aplicaci√≥n sea accesible desde el exterior (por ejemplo, desde un navegador en internet), es necesario que la m√°quina donde est√° desplegado el cl√∫ster tenga una IP p√∫blica expuesta y que se encuentren configuradas reglas de red o de firewall (Security Groups) que permitan el acceso externo a los puertos asignados (como el 30593).
Dado que no se cuenta con acceso administrativo a la infraestructura de red en la nube, no fue posible configurar la exposici√≥n externa de manera segura. Esta limitaci√≥n fue considerada para no comprometer la estabilidad del entorno ni interferir con la configuraci√≥n existente del proveedor.

